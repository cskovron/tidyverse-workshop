---
title: "Introduction to the tidyverse: Data analysis with `infer` and `broom`"
author: |
        | Christopher Skovron
        | Northwestern University 
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: 
  revealjs::revealjs_presentation: 
    theme: simple
    highlight: haddock
    center: false
    transition: none
    #css: reveal.css
    self_contained: false
---



## The workflow: modeling 

![](../images/data-science-explore.png)

## How do I do tidy data analysis? 

Everything we've seen so far has been relatively complete. The final part of the tidyverse that's under the most development is modeling. 



**Note**: The `type` argument in `generate()` is automatically filled based on the entries for `specify()` and
`hypothesize()`. It can be removed throughout the examples that follow. It is left in to reiterate the type of generation process being performed.

## Data preparation

```{r message=FALSE, warning=FALSE}
library(nycflights13)
library(tidyverse)
library(infer)
set.seed(2017)
fli_small <- flights %>% 
  sample_n(size = 500) %>% 
  mutate(half_year = case_when(
    between(month, 1, 6) ~ "h1",
    between(month, 7, 12) ~ "h2"
  )) %>% 
  mutate(day_hour = case_when(
    between(hour, 1, 12) ~ "morning",
    between(hour, 13, 24) ~ "not morning"
  )) %>% 
  select(arr_delay, dep_delay, half_year, 
         day_hour, origin, carrier)
```

## Data preparation

* Two numeric - `arr_delay`, `dep_delay`
* Two categories 
    - `half_year` (`"h1"`, `"h2"`), 
    - `day_hour` (`"morning"`, `"not morning"`)
* Three categories - `origin` (`"EWR"`, `"JFK"`, `"LGA"`)
* Sixteen categories - `carrier`

***


## Calculate observed statistic 

The recommended approach is to use `specify() %>% calculate()`:

```{r}
obs_t <- fli_small %>%
  specify(arr_delay ~ half_year) %>%
  calculate(stat = "t", order = c("h1", "h2"))
```

The observed $t$ statistic is `r obs_t`.


## Calculate observed statistic 

Or using `t_test` in `infer`

```{r}
obs_t <- fli_small %>% 
  t_test(formula = arr_delay ~ half_year, alternative = "two_sided",
         order = c("h1", "h2")) %>% 
  dplyr::select(statistic) %>% 
  dplyr::pull()
```

The observed $t$ statistic is `r obs_t`.


## Calculate observed statistic 

Or using another shortcut function in `infer`:

```{r}
obs_t <- fli_small %>% 
  t_stat(formula = arr_delay ~ half_year, order = c("h1", "h2"))
```

The observed $t$ statistic is `r obs_t`.


## Data preparation - chi-square

```{r message=FALSE, warning=FALSE}
library(nycflights13)
library(tidyverse)
library(infer)
set.seed(2017)
fli_small <- flights %>% 
  na.omit() %>% 
  sample_n(size = 500) %>% 
  mutate(season = case_when(
    month %in% c(10:12, 1:3) ~ "winter",
    month %in% c(4:9) ~ "summer"
  )) %>% 
  mutate(day_hour = case_when(
    between(hour, 1, 12) ~ "morning",
    between(hour, 13, 24) ~ "not morning"
  )) %>% 
  select(arr_delay, dep_delay, season, 
         day_hour, origin, carrier)
```


## Calculate observed statistic 

The recommended approach is to use `specify() %>% calculate()`:

```{r}
obs_chisq <- fli_small %>%
  specify(origin ~ season) %>% # alt: response = origin, explanatory = season
  calculate(stat = "Chisq")
```

The observed $\chi^2$ statistic is `r obs_chisq`.


## Calculate observed statistic 

Or using `chisq_test` in `infer`

```{r}
obs_chisq <- fli_small %>% 
  chisq_test(formula = origin ~ season) %>% 
  dplyr::select(statistic)
```

Again, the observed $\chi^2$ statistic is `r obs_chisq`.

## Calculate observed statistic 

Or using another shortcut function in `infer`:

```{r}
obs_chisq <- fli_small %>% 
  chisq_stat(formula = origin ~ season)
```

Lastly, the observed $\chi^2$ statistic is `r obs_chisq`.


```{r setup, echo=FALSE}
library(knitr)
opts_chunk$set(warning=FALSE, message=FALSE)
```

## broom: let's tidy up a bit


The broom package takes the messy output of built-in functions in R, such as `lm`, `nls`, or `t.test`, and turns them into tidy data frames.


## Tidying functions

* `tidy`: constructs a data frame that summarizes the model's statistical findings. This includes coefficients and p-values for each term in a regression, per-cluster information in clustering applications, or per-test information for `multtest` functions.
* `augment`: add columns to the original data that was modeled. This includes predictions, residuals, and cluster assignments.
* `glance`: construct a concise *one-row* summary of the model. This typically contains values such as R^2, adjusted R^2, and residual standard error that are computed once for the entire model.


## Example 

```{r lmfit}
lmfit <- lm(mpg ~ wt, mtcars)
lmfit
summary(lmfit)
```

## Tidy the output 

Instead, you can use the `tidy` function, from the broom package, on the fit:

```{r}
library(broom)
tidy(lmfit)
```

## Tidy the output 

This gives you a data.frame representation. Note that the row names have been moved into a column called `term`, and the column names are simple and consistent (and can be accessed using `$`).

## Augment the output 

Instead of viewing the coefficients, you might be interested in the fitted values and residuals for each of the original points in the regression. For this, use `augment`, which augments the original data with information from the model:

```{r}
augment(lmfit)
```

Note that each of the new columns begins with a `.` (to avoid overwriting any of the original columns).

## Glance at the output 


Finally, several summary statistics are computed for the entire regression, such as R^2 and the F-statistic. These can be accessed with the `glance` function:

```{r}
glance(lmfit)
```



## Generalized linear and non-linear models

These functions apply equally well to the output from `glm`:

```{r glmfit}
glmfit <- glm(am ~ wt, mtcars, family="binomial")
tidy(glmfit)
augment(glmfit)
glance(glmfit)
```

## Generalized linear and non-linear models


These functions also work on other fits, such as nonlinear models (`nls`):

```{r}
nlsfit <- nls(mpg ~ k / wt + b, mtcars, start=list(k=1, b=0))
tidy(nlsfit)
augment(nlsfit, mtcars)
glance(nlsfit)
```

## Hypothesis testing

The `tidy` function can also be applied to `htest` objects, such as those output by popular built-in functions like `t.test`, `cor.test`, and `wilcox.test`.

```{r ttest}
tt <- t.test(wt ~ am, mtcars)
tidy(tt)
```

## Hypothesis testing

Some cases might have fewer columns (for example, no confidence interval):

```{r}
wt <- wilcox.test(wt ~ am, mtcars)
tidy(wt)
```

## Hypothesis testing

Since the `tidy` output is already only one row, `glance` returns the same output:

```{r}
glance(tt)
glance(wt)
```



## broom and dplyr

While broom is useful for summarizing the result of a single analysis in a consistent format, it is really designed for high-throughput applications, where you must combine results from multiple analyses. These could be subgroups of data, analyses using different models, bootstrap replicates, permutations, and so on. In particular, it plays well with the `nest/unnest` functions in `tidyr` and the `map` function in `purrr`.

Let's try this on a simple dataset, the built-in `Orange`. We start by coercing `Orange` to a `tibble`. This gives a nicer print method that will especially useful later on when we start working with list-columns.

```{r}
library(broom)
library(tibble)

data(Orange)

Orange <- as_tibble(Orange)
Orange
```

This contains 35 observations of three variables: `Tree`, `age`, and `circumference`. `Tree` is a factor with five levels describing five trees. As might be expected, age and circumference are correlated:

```{r}
cor(Orange$age, Orange$circumference)

library(ggplot2)

ggplot(Orange, aes(age, circumference, color = Tree)) +
  geom_line()
```

Suppose you want to test for correlations individually *within* each tree. You can do this with dplyr's `group_by`:

```{r}
library(dplyr)

Orange %>% 
  group_by(Tree) %>%
  summarize(correlation = cor(age, circumference))
```

(Note that the correlations are much higher than the aggregated one, and furthermore we can now see it is similar across trees).

Suppose that instead of simply estimating a correlation, we want to perform a hypothesis test with `cor.test`:

```{r}
ct <- cor.test(Orange$age, Orange$circumference)
ct
```

This contains multiple values we could want in our output. Some are vectors of length 1, such as the p-value and the estimate, and some are longer, such as the confidence interval. We can get this into a nicely organized tibble using the `tidy` function:

```{r}
tidy(ct)
```

Often, we want to perform multiple tests or fit multiple models, each on a different part of the data. In this case, we recommend a `nest-map-unnest` workflow. For example, suppose we want to perform correlation tests for each different tree. We start by `nest`ing our data based on the group of interest:

```{r}
library(tidyr)
library(purrr)

nested <- Orange %>% 
  nest(-Tree)
```

Then we run a correlation test for each nested tibble using `purrr::map`:

```{r}
nested %>% 
  mutate(test = map(data, ~ cor.test(.x$age, .x$circumference)))
```

This results in a list-column of S3 objects. We want to tidy each of the objects, which we can also do with `map`.

```{r}
nested %>% 
  mutate(
    test = map(data, ~ cor.test(.x$age, .x$circumference)), # S3 list-col
    tidied = map(test, tidy)
  ) 
```

Finally, we want to unnest the tidied data frames so we can see the results in a flat tibble. All together, this looks like:

```{r}
Orange %>% 
  nest(-Tree) %>% 
  mutate(
    test = map(data, ~ cor.test(.x$age, .x$circumference)), # S3 list-col
    tidied = map(test, tidy)
  ) %>% 
  unnest(tidied, .drop = TRUE)
```

Note that the `.drop` argument to `tidyr::unnest` is often useful.

This workflow becomes even more useful when applied to regressions. Untidy ouput for a regression looks like:

```{r}
lm_fit <- lm(age ~ circumference, data = Orange)
summary(lm_fit)
```

where we tidy these results, we get multiple rows of output for each model:

```{r}
tidy(lm_fit)
```

Now we can handle multiple regressions at once using exactly the same workflow as before:

```{r}
Orange %>%
  nest(-Tree) %>% 
  mutate(
    fit = map(data, ~ lm(age ~ circumference, data = .x)),
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied)
```

You can just as easily use multiple predictors in the regressions, as shown here on the `mtcars` dataset. We nest the data into automatic and manual cars (the `am` column), then perform the regression within each nested tibble.

```{r}
data(mtcars)
mtcars <- as_tibble(mtcars)  # to play nicely with list-cols
mtcars

mtcars %>%
  nest(-am) %>% 
  mutate(
    fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)),  # S3 list-col
    tidied = map(fit, tidy)
  ) %>% 
  unnest(tidied)
```

What if you want not just the `tidy` output, but the `augment` and `glance` outputs as well, while still performing each regression only once? Since we're using list-columns, we can just fit the model once and use multiple list-columns to store the tidied, glanced and augmented outputs.

```{r}
regressions <- mtcars %>%
  nest(-am) %>% 
  mutate(
    fit = map(data, ~ lm(wt ~ mpg + qsec + gear, data = .x)),
    tidied = map(fit, tidy),
    glanced = map(fit, glance),
    augmented = map(fit, augment)
  )

regressions %>% 
  unnest(tidied)

regressions %>% 
  unnest(glanced, .drop = TRUE)

regressions %>% 
  unnest(augmented)
```

By combining the estimates and p-values across all groups into the same tidy data frame (instead of a list of output model objects), a new class of analyses and visualizations becomes straightforward. This includes

- Sorting by p-value or estimate to find the most significant terms across all tests
- P-value histograms
- Volcano plots comparing p-values to effect size estimates

In each of these cases, we can easily filter, facet, or distinguish based on the `term` column. In short, this makes the tools of tidy data analysis available for the *results* of data analysis and models, not just the inputs.

